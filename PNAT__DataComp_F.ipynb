{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PNAT__DataComp",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHn5aNLg23vp"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?export=view&id=1DEpMpgsX-MU3-de4Gqoa7Nk3VD12_Vwk)\n",
        "\n",
        "# üìåüìåüìå This is a notebook for the [Data-Competition](https://github.com/fsoft-ailab/Data-Competition), which is hosted by FPT Software's AILab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zWgoG1OfYf_"
      },
      "source": [
        "# ‚öôÔ∏è Setup\n",
        "\n",
        "Clone repo, install dependencies and check PyTorch and GPU."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1TNdfgXOH08ZiW0p-_w0IITBMdVvd_sub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIZe-nQQeiNU",
        "outputId": "a5fef2b8-4c83-4bb3-da8a-78c3013d6d9a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1TNdfgXOH08ZiW0p-_w0IITBMdVvd_sub\n",
            "To: /content/Data-Competition.zip\n",
            "100% 358M/358M [00:01<00:00, 202MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/Data-Competition.zip"
      ],
      "metadata": {
        "id": "otbue8gVl-h2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fd297db-6e20-49ce-ab6f-03e772fdf7d4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/Data-Competition.zip\n",
            "replace Data-Competition/.git/config? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace Data-Competition/.git/description? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n",
            "  inflating: Data-Competition/Calculating_mAP/output/10.35.17.103_01_20210709172732794_MOTION_DETECTION.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihGOYm8DfJsl",
        "outputId": "880d89ee-4bcc-4e92-bc55-9671f3c34b62"
      },
      "source": [
        "# %cd /content/drive/MyDrive\n",
        "# !git clone https://github.com/fsoft-ailab/Data-Competition # clone repo\n",
        "%cd /content/Data-Competition\n",
        "!pip3 install -r requirements.txt # install dependencies\n",
        "\n",
        "import torch\n",
        "from IPython.display import Image, clear_output\n",
        "\n",
        "clear_output()\n",
        "print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete. Using torch 1.10.0+cu111 (Tesla K80)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Web_mcnefe5k"
      },
      "source": [
        "# üìÇ Download Dataset\n",
        "\n",
        "\n",
        "\n",
        "> üî∞ Download the original dataset. If you want to use your own data, upload it to google drive and replace the file's id in the scripts below.\n",
        "\n",
        "\n",
        "> üî∞ We suggest following the organizer's structure. This will be convenient during the procedure.\n",
        "\n",
        "\n",
        "üí° If you do not know how to get the file's id, click [here](https://www.swipetips.com/how-to-determine-the-file-id-of-a-content-in-google-drive/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSlDXj8__228"
      },
      "source": [
        "# import gdown\n",
        "# import os\n",
        "\n",
        "# id = '13DNROVhn0K2L2aeDoSqU2OrgozsTH45o' # file's id (change your file's id)\n",
        "\n",
        "# url = 'https://drive.google.com/u/1/uc?id={}&export=download'.format(id)\n",
        "# output = './dataset'\n",
        "# gdown.download(url, output, quiet=False)\n",
        "# # !unzip dataset.zip\n",
        "# # os.remove('./dataset.zip')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQQN8y0CnVMv"
      },
      "source": [
        "# üî¨ Trainning \n",
        "\n",
        "\n",
        "\n",
        "> üî∞ We configured all the parameters for training\n",
        "\n",
        "\n",
        "> üî∞ If the structure of the dataset is not the same as the organizer, you can change the path in `config/data_cfg.yaml`\n",
        "\n",
        "\n",
        "‚ùó‚ùó You should not change such hyperparameters\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/Data-Competition/dataset/images/val/* /content/Data-Competition/dataset/images/train/\n",
        "!cp /content/Data-Competition/dataset/labels/val/* /content/Data-Competition/dataset/labels/train/"
      ],
      "metadata": {
        "id": "snTi-WtcmEwr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/Data-Competition"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCwOz1N6NJZn",
        "outputId": "138e30d2-b470-4276-d3b7-965c1bf1f0d3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Data-Competition\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mD4kdBZCBtT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "774db749-a713-4eb1-9e0a-f69fdeb832dc"
      },
      "source": [
        "!python train.py --name test"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mTrain: \u001b[0mdata_cfg=config/data_cfg.yaml, batch_size=64, cache=None, device=, workers=8, name=test, weights=pretrains/pretrain.pt, model_cfg=models/yolov5s.yaml, hyp=config/hyps/hyp_finetune.yaml, project=results/train, artifact_alias=latest, epochs=100, img_size=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, evolve=False, bucket=, image_weights=False, multi_scale=False, single_cls=False, adam=False, sync_bn=False, entity=, exist_ok=False, quad=False, label_smoothing=0.0, linear_lr=False, bbox_interval=-1, save_period=-1, patience=100\n",
            "YOLOv5 üöÄ 63741b6 torch 1.10.0+cu111 CUDA:0 (Tesla K80, 11441.1875MB)\n",
            "\n",
            "cuda:0\n",
            "\u001b[34m\u001b[1mHyper parameters: \u001b[0mlr0=0.0032, lrf=0.2, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.0, hsv_s=0.0, hsv_v=0.0, degrees=0.0, translate=0.0, scale=0.0, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.0, mosaic=0.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir results/train', view at http://localhost:6006/\n",
            "Overriding model.yaml nc=80 with nc=3\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Focus                     [3, 32, 3]                    \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  3    156928  models.common.C3                        [128, 128, 3]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1    656896  models.common.SPP                       [512, 512, [5, 9, 13]]        \n",
            "  9                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1     21576  models.yolo.Detect                      [3, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "/usr/local/lib/python3.7/dist-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "Model Summary: 283 layers, 7068936 parameters, 7068936 gradients, 16.4 GFLOPs\n",
            "\n",
            "Transferred 360/362 items from pretrains/pretrain.pt\n",
            "Scaled weight_decay = 0.0005\n",
            "\u001b[34m\u001b[1mOptimizer:\u001b[0m SGD with parameter groups 59 weight, 62 weight (no decay), 62 bias\n",
            "\u001b[34m\u001b[1mTrain: \u001b[0mScanning 'dataset/labels/train' images and labels...976 found, 0 missing, 0 empty, 0 corrupted: 100% 976/976 [00:01<00:00, 960.18it/s]\n",
            "\u001b[34m\u001b[1mTrain: \u001b[0mNew cache created: dataset/labels/train.cache\n",
            "\u001b[34m\u001b[1mVal: \u001b[0mScanning 'dataset/labels/val.cache' images and labels... 976 found, 0 missing, 0 empty, 0 corrupted: 100% 976/976 [00:00<?, ?it/s]\n",
            "Plotting labels... \n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 426, in <module>\n",
            "    main(args=parser())\n",
            "  File \"train.py\", line 422, in main\n",
            "    train(args.hyp, args, device, callbacks)\n",
            "  File \"train.py\", line 207, in train\n",
            "    plot_labels(labels, class_name, save_dir)\n",
            "  File \"/content/Data-Competition/utils/plots.py\", line 293, in plot_labels\n",
            "    sn.pairplot(x, corner=True, diag_kind='auto', kind='hist', diag_kws=dict(bins=50), plot_kws=dict(pmax=0.9))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py\", line 46, in inner_f\n",
            "    return f(**kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/seaborn/axisgrid.py\", line 2098, in pairplot\n",
            "    height=height, aspect=aspect, dropna=dropna, **grid_kws)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py\", line 46, in inner_f\n",
            "    return f(**kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/seaborn/axisgrid.py\", line 1326, in __init__\n",
            "    self.tight_layout(pad=layout_pad)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/seaborn/axisgrid.py\", line 88, in tight_layout\n",
            "    self._figure.tight_layout(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/matplotlib/cbook/deprecation.py\", line 358, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/matplotlib/figure.py\", line 2496, in tight_layout\n",
            "    pad=pad, h_pad=h_pad, w_pad=w_pad, rect=rect)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/matplotlib/tight_layout.py\", line 388, in get_tight_layout_figure\n",
            "    rect=(left, bottom, right, top))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/matplotlib/tight_layout.py\", line 109, in auto_adjust_subplotpars\n",
            "    tight_bbox_raw = union([ax.get_tightbbox(renderer) for ax in subplots\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/matplotlib/tight_layout.py\", line 110, in <listcomp>\n",
            "    if ax.get_visible()])\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_base.py\", line 4331, in get_tightbbox\n",
            "    self._update_title_position(renderer)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_base.py\", line 2533, in _update_title_position\n",
            "    bb = ax.xaxis.get_tightbbox(renderer)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/matplotlib/axis.py\", line 1188, in get_tightbbox\n",
            "    self._update_label_position(renderer)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/matplotlib/axis.py\", line 2028, in _update_label_position\n",
            "    spine.get_path()).get_extents()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/matplotlib/transforms.py\", line 1518, in transform_path\n",
            "    return self.transform_path_affine(self.transform_path_non_affine(path))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/matplotlib/transforms.py\", line 1543, in transform_path_non_affine\n",
            "    return Path._fast_from_codes_and_verts(x, path.codes, path)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/matplotlib/path.py\", line 175, in _fast_from_codes_and_verts\n",
            "    pth._vertices = _to_unmasked_float_array(verts)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/matplotlib/cbook/__init__.py\", line 1317, in _to_unmasked_float_array\n",
            "    return np.asarray(x, float)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py\", line 83, in asarray\n",
            "    return array(a, dtype, copy=False, order=order)\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZoDLd6jpWbD"
      },
      "source": [
        "# ‚úç Evaluation\n",
        "\n",
        "\n",
        "> üî∞ val.py evaluates a model on a particular dataset. There are three different types of mAP@.5. To rank, we use wAP and the formula mentioned on [github](https://github.com/fsoft-ailab/Data-Competition#model--metrics-).\n",
        "\n",
        "\n",
        "\n",
        "> üî∞ The results are saved in ***results/evaluate*** once the process is finished.\n",
        "\n",
        "\n",
        "\n",
        "```shell\n",
        "python3 val.py --weights <path_to_weight> --task test --name <version_name> --batch-size 64 --device 0\n",
        "                                                 val\n",
        "                                                 train\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GcItgqQCVCX"
      },
      "source": [
        "!python val.py --weights /content/Data-Competition/results/train/test10/weights/best.pt --task val --name test --device 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69npbzNLpZ-V"
      },
      "source": [
        "# üîç Detection\n",
        "\n",
        "\n",
        "\n",
        "> üî∞  The model can be inferred from two sources: an image or a folder of images.\n",
        "\n",
        "\n",
        "\n",
        "> üî∞  If you want to get bounding box, hide conf, ... look for argument in `detect.py`\n",
        "\n",
        "```shell\n",
        "python detect.py --weights <your_weights> --source <path_to_image>\n",
        "                                                   <path_to_folder>\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcDa0NRwQl0m"
      },
      "source": [
        "#!python detect.py --weights /content/drive/MyDrive/Data-Competition/results/train/test10/weights/best.pt --source dataset/images/val --dir ./detect_public_test\n",
        "#!python detect.py --weights results/train/test/weights/best.pt --source dataset/images/public_test --dir ./detect_public_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxIc3QQtUh7G"
      },
      "source": [
        "#Image(filename='/content/drive/MyDrive/Data-Competition/detect_public_test5/10.35.17.101_01_20210712140351896_MOTION_DETECTION.jpg', width=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate wAP "
      ],
      "metadata": {
        "id": "xcS1bMAfaSZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/constanreedjohn/Calculating_mAP.git"
      ],
      "metadata": {
        "id": "uZWFAHFMaUgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/Data-Competition/Calculating_mAP"
      ],
      "metadata": {
        "id": "htL63wFO1ZbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from posixpath import splitext\n",
        "import os \n",
        "import os.path\n",
        "import shutil\n",
        "\n",
        "count = 0\n",
        "count_t = 0\n",
        "\n",
        "path2val     = '/content/Data-Competition/dataset/labels/val'\n",
        "path2val_img = '/content/Data-Competition/dataset/images/val'\n",
        "\n",
        "path2pred = \"/content/Data-Competition/results/evaluate/val/test2/labels\" #change path to get save_txt from evalutation\n",
        "\n",
        "gt_dest   = \"/content/Data-Competition/Calculating_mAP/input/temp_gt\"\n",
        "pred_dest = \"/content/Data-Competition/Calculating_mAP/input/temp_pred\"\n",
        "\n",
        "path2train_lab = '/content/Data-Competition/dataset/labels/train'\n",
        "path2train_img = '/content/Data-Competition/dataset/images/train'\n",
        "continue_training = True\n",
        "while continue_training == True:\n",
        "  count = 0\n",
        "  count_t = 0\n",
        "  !cd /content/Data-Competition/Calculating_mAP\n",
        "  continue_training = False #Flag for stop training iteration with condition: all wAP > 50 and all images are detected\n",
        "  list_val = os.listdir(path2val) # dir is your directory path\n",
        "  list_result = os.listdir(path2pred)\n",
        "  number_files_val = len(list_val)\n",
        "  number_files_result = len(list_result)\n",
        "  \n",
        "\n",
        "  for file in os.listdir(path2val):\n",
        "    \n",
        "    gt_path = os.path.join(path2val, file)\n",
        "    pred_path = os.path.join(path2pred, file)\n",
        "\n",
        "    print(\"{} / {}\\n\".format(count,number_files_result))\n",
        "    count += 1\n",
        "    if os.path.exists(pred_path):\n",
        "      shutil.copy(gt_path,gt_dest + \"/\"+ file)\n",
        "      shutil.copy(pred_path,pred_dest + \"/\"+ file)\n",
        "      !python /content/Data-Competition/Calculating_mAP/Format_coverter.py\n",
        "      !python /content/Data-Competition/Calculating_mAP/cp.py\n",
        "      !python /content/Data-Competition/Calculating_mAP/main.py\n",
        "    else:\n",
        "      #Copy to train and retrain if detection doesnt detect anything\n",
        "      shutil.copy(gt_path, path2train_lab + \"/\" + file)\n",
        "      name = splitext(file)[0]\n",
        "      shutil.copy(path2val_img + \"/\" + name + \".jpg\", path2train_img + \"/\" + name + \".jpg\")\n",
        "      print(file + \" has been moved to train\")\n",
        "      count_t += 1\n",
        "      continue_training = True\n",
        "      continue\n",
        "\n",
        "    # Using readlines()\n",
        "    file1 = open('/content/Data-Competition/Calculating_mAP/output/' + file, 'r')\n",
        "    Lines = file1.readlines()\n",
        "    # If wAP < 50 then cp to train \n",
        "    for line in Lines:\n",
        "      if line[0] == \"w\":\n",
        "        metric = float(line[6:-2])\n",
        "        break\n",
        "    if metric < 50:\n",
        "      shutil.copy(gt_path, path2train_lab + \"/\" + file)\n",
        "      name = splitext(file)[0]\n",
        "      shutil.copy(path2val_img + \"/\" + name + \".jpg\", path2train_img + \"/\" + name + \".jpg\")\n",
        "      print(file + \" has been moved to train\")\n",
        "      count_t += 1\n",
        "      continue_training = True\n",
        "  print(\"Number of train sample is: \",count_t)\n",
        "  !cd /content/Data-Competition\n",
        "  !python /content/Data-Competition/train.py --name test"
      ],
      "metadata": {
        "id": "IOJscWpnYQLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cp -r /content/Data-Competition/dataset /content/drive/MyDrive/Data-Competition"
      ],
      "metadata": {
        "id": "9qcXj7RYDXz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!cp -r /content/drive/MyDrive/Data-Competition/dataset /content/Data-Competition"
      ],
      "metadata": {
        "id": "pDdV89XOP8lM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weak Augmentation\n"
      ],
      "metadata": {
        "id": "JEnnamRkYXuO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import imgaug.augmenters as iaa"
      ],
      "metadata": {
        "id": "F0enehu6Yb8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/Data-Competition"
      ],
      "metadata": {
        "id": "tX9liD3TY3QO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import imgaug as ia\n",
        "import imgaug.augmenters as iaa\n",
        "from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "path = '/content/Data-Competition/dataset/images/train/'\n",
        "path2label = '/content/Data-Competition/dataset/labels/train/'\n",
        "path2labelwrite = '/content/Data-Competition/dataset/labels/train/'\n",
        "directory = r'/content/Data-Competition/dataset/images/train'\n",
        "for file in os.listdir(path):\n",
        "  # Load the image\n",
        "  classs = []\n",
        "  x1 = []\n",
        "  x2 = []\n",
        "  y1 = []\n",
        "  y2 = []\n",
        "  output = \"\"\n",
        "  print(file)\n",
        "  image = cv2.imread(path + file) \n",
        "  os.chdir(path2label)\n",
        "  file1 = open(path2label + os.path.splitext(file)[0] + '.txt', 'r')\n",
        "  Lines = file1.readlines()\n",
        " \n",
        "  count = 0\n",
        "  # Strips the newline character\n",
        "  for line in Lines:\n",
        "    count += 1\n",
        "    lines = line.strip()\n",
        "    classes, x, y, w, h = lines.split()\n",
        "\n",
        "\n",
        "    xo = round(float(x),6) - round(round(float(w),6)/2,6)\n",
        "    yo = round(float(y),6) + round(round(float(h),6)/2,6)\n",
        "    xt = round(float(x),6) + round(round(float(w),6)/2,6)\n",
        "    yt = round(float(y),6) - round(round(float(h),6)/2,6)\n",
        "    \n",
        "    classs.append(classes)\n",
        "    x1.append(round(xo,6))\n",
        "    x2.append(round(xt,6))\n",
        "    y1.append(round(yo,6))\n",
        "    y2.append(round(yt,6))\n",
        "  ia.seed(1)\n",
        "  for idx in range(count):\n",
        "    bbs = BoundingBoxesOnImage([\n",
        "        BoundingBox(x1=x1[idx] * image.shape[1], y1=y1[idx] * image.shape[0], x2=x2[idx] * image.shape[1], y2=y2[idx] * image.shape[0])\n",
        "    ], shape=image.shape)\n",
        "    seq = iaa.Sequential([\n",
        "        iaa.Fliplr(0.5)\n",
        "    ])\n",
        "\n",
        "    # Augment BBs and images.\n",
        "    image_aug, bbs_aug = seq(image=image, bounding_boxes=bbs)\n",
        "\n",
        "    # print coordinates before/after augmentation (see below)\n",
        "    # use .x1_int, .y_int, ... to get integer coordinates\n",
        "    after = bbs_aug.bounding_boxes[0]\n",
        "    x = round(((after.x1 + after.x2)/2)/image.shape[1],6)\n",
        "    y = round(((after.y1 + after.y2)/2)/image.shape[0],6)\n",
        "    width = round((after.x2 - after.x1)/image.shape[1],6)\n",
        "    height = round((after.y2 - after.y1)/image.shape[0],6)\n",
        "    output += classs[idx] + \" \" + str(x) + \" \" + str(y) + \" \" + str(width) + \" \" + str(height) + \"\\n\"\n",
        "  \n",
        "  text_file = open(path2labelwrite + os.path.splitext(file)[0] + \"_Aug\" + '.txt', \"w\")\n",
        "  text_file.write(output)\n",
        "  text_file.close()\n",
        "  os.chdir(directory)\n",
        "  cv2.imwrite(file + \"_Aug\", image_aug)"
      ],
      "metadata": {
        "id": "8ax0j_STZMQs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}